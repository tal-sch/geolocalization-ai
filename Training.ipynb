{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84b4d245",
   "metadata": {},
   "source": [
    "# Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fee74d4",
   "metadata": {
    "id": "1fee74d4"
   },
   "outputs": [],
   "source": [
    "# Makes sure to reload modules when they change\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# --- Standard Library Imports ---\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "# --- Third-Party Imports ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.amp import GradScaler, autocast\n",
    "\n",
    "import torch\n",
    "\n",
    "# --- Custom Imports ---\n",
    "from src.utils import (\n",
    "    haversine_distance,\n",
    "    plot_images_from_dataloader,\n",
    "    setup_TensorBoard_writers,\n",
    ")\n",
    "\n",
    "from src.dataset import GeolocalizationDataset\n",
    "from src.models import MultiTaskDINOGeo\n",
    "\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979e38b5",
   "metadata": {},
   "source": [
    "# Data loading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b3188d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    images_path = \"dataset_root/images/\"\n",
    "    coords_csv_path = \"dataset_root/gt.csv\"\n",
    "    SCALER_SAVE_PATH = \"coordinate_scaler.pkl\"\n",
    "\n",
    "    coords_df = pd.read_csv(coords_csv_path)\n",
    "    coords_df.columns = ['filename', 'lat', 'lon']\n",
    "\n",
    "    coords_df['path'] = coords_df['filename'].apply(lambda x: os.path.join(images_path, x))\n",
    "    coords_df = coords_df[['path', 'lat', 'lon']]\n",
    "\n",
    "    train_df, val_df = train_test_split(coords_df, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Scale latitude and longitude to [0, 1] range\n",
    "    # must fit on training data *only*\n",
    "    scaler = MinMaxScaler()\n",
    "    train_df[[\"lat\", \"lon\"]] = scaler.fit_transform(train_df[[\"lat\", \"lon\"]])\n",
    "    val_df[[\"lat\", \"lon\"]] = scaler.transform(val_df[[\"lat\", \"lon\"]])\n",
    "\n",
    "    joblib.dump(scaler, SCALER_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cfa384",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf405cb2",
   "metadata": {},
   "source": [
    "## Clustering for multi-task training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548cacad",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ZONES = 25 \n",
    "\n",
    "print(\"Generating Zone Labels on Normalized Data...\")\n",
    "kmeans = KMeans(n_clusters=NUM_ZONES, random_state=42, n_init=10)\n",
    "\n",
    "# Fit on the normalized coordinates\n",
    "train_df['zone_label'] = kmeans.fit_predict(train_df[['lat', 'lon']])\n",
    "val_df['zone_label'] = kmeans.predict(val_df[['lat', 'lon']])\n",
    "\n",
    "# plot the zones to see if they look reasonable for both training and validation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Training data plot\n",
    "axes[0].scatter(train_df['lon'], train_df['lat'], c=train_df['zone_label'], cmap='tab20', s=30)\n",
    "axes[0].set_title(f\"Training Data Divided into {NUM_ZONES} Zones\")\n",
    "axes[0].axis('equal')  # Keep aspect ratio so it looks like a map\n",
    "\n",
    "# Validation data plot\n",
    "axes[1].scatter(val_df['lon'], val_df['lat'], c=val_df['zone_label'], cmap='tab20', s=30)\n",
    "axes[1].set_title(f\"Validation Data Divided into {NUM_ZONES} Zones\")\n",
    "axes[1].axis('equal')  # Keep aspect ratio so it looks like a map\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fd8cab",
   "metadata": {},
   "source": [
    "# Data loading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7MXnRC5rRo8v",
   "metadata": {
    "id": "7MXnRC5rRo8v"
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"Initializing train dataset...\")\n",
    "    train_dataset = GeolocalizationDataset(\n",
    "        image_paths=train_df[\"path\"].tolist(),\n",
    "        coordinates=train_df[[\"lat\", \"lon\"]].values,\n",
    "        zone_labels=train_df[\"zone_label\"].values,\n",
    "        is_train=True,\n",
    "        target_size=(252, 182),\n",
    "    )\n",
    "    print(\"Initializing validation dataset...\")\n",
    "    val_dataset = GeolocalizationDataset(\n",
    "        image_paths=val_df[\"path\"].tolist(),\n",
    "        coordinates=val_df[[\"lat\", \"lon\"]].values,\n",
    "        zone_labels=val_df[\"zone_label\"].values,\n",
    "        is_train=False,\n",
    "        target_size=(252, 182),\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=64, shuffle=True, num_workers=0, pin_memory=True\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, batch_size=128, shuffle=False, num_workers=0, pin_memory=False\n",
    "    )\n",
    "\n",
    "    plot_images_from_dataloader(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14bbed1",
   "metadata": {},
   "source": [
    "# Model setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb34c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "is_rtx = \"RTX\" in torch.cuda.get_device_name(0)\n",
    "print(\"Using device:\", device)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "model = MultiTaskDINOGeo(NUM_ZONES).to(device)\n",
    "\n",
    "if is_rtx:\n",
    "    model = model.to(\n",
    "        memory_format=torch.channels_last\n",
    "    )  # Optimize for modern GPUs that prefer channels_last\n",
    "else:\n",
    "    print(\"RTX card not detected: Disabling AMP/Channels_Last optimizations\")\n",
    "    model = model.to(device)\n",
    "\n",
    "# Training Hyperparameters\n",
    "UNFREEZE_INTERVAL = 20  # epochs between unfreezing backbone blocks\n",
    "BLOCKS_PER_STEP = 1     # How many blocks to open at once\n",
    "victory_lap_started = False\n",
    "\n",
    "patience_counter = 0\n",
    "early_stopping_patience = UNFREEZE_INTERVAL + 1 # stop early if no improvement before next unfreeze\n",
    "epochs = 300\n",
    "\n",
    "use_TensorBoard = True  # Set to False to disable TensorBoard logging\n",
    "\n",
    "criterion_reg = torch.nn.HuberLoss(delta=1.0)\n",
    "criterion_cls = torch.nn.CrossEntropyLoss(label_smoothing=0.15)\n",
    "\n",
    "# Learning rates for phase 1 - iterative unfreezing\n",
    "p1_base_head_lr = 1e-3\n",
    "p1_backbone_lr = 5e-5\n",
    "\n",
    "# Learning rates for phase 2 - fine-tuning all layers\n",
    "p2_backbone_lr = 1e-5  \n",
    "p2_head_lr = 5e-4  \n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    [\n",
    "        {   \n",
    "            \"params\": filter(lambda p: p.requires_grad, model.backbone.parameters()),\n",
    "            \"lr\": p1_backbone_lr,\n",
    "        },\n",
    "        {\"params\": model.shared.parameters(), \"lr\": p1_base_head_lr},\n",
    "        {\"params\": model.reg_head.parameters(), \"lr\": p1_base_head_lr},\n",
    "        {\"params\": model.cls_head.parameters(), \"lr\": p1_base_head_lr},\n",
    "    ],\n",
    "    weight_decay=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    optimizer, T_0=UNFREEZE_INTERVAL, T_mult=1, eta_min=1e-7)\n",
    "\n",
    "print(f\"Training on {len(train_dataset)} images, Validating on {len(val_dataset)} images.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83984718",
   "metadata": {},
   "source": [
    "# Model training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99a6546",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# For tracking metrics\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_avg_dist_history = []\n",
    "val_median_dist_history = []\n",
    "val_zone_accuracy = []\n",
    "learning_rates = []\n",
    "\n",
    "# for saving the best model\n",
    "best_dist = float(\"inf\")\n",
    "best_epoch = -1\n",
    "best_model_path = \"trained_models/geo_model_training_in_progress.pth\"\n",
    "\n",
    "if use_TensorBoard:\n",
    "    writer_train, writer_val = setup_TensorBoard_writers()\n",
    "\n",
    "print(f\"Starting training on {device}...\")\n",
    "\n",
    "# scaler for mixed precision training, prevents gradient underflow\n",
    "gradScaler = GradScaler(\"cuda\")  \n",
    "\n",
    "try:\n",
    "    for epoch in range(epochs):\n",
    "        # Check for Unfreezing layer blocks\n",
    "        if epoch > 0 and epoch % UNFREEZE_INTERVAL == 0 and not victory_lap_started:\n",
    "            model.unfreeze_step(BLOCKS_PER_STEP)\n",
    "\n",
    "            # Check if all blocks but the first are unfrozen - phase 2 begins\n",
    "            # unfreeze the first block too and start fine-tuning\n",
    "            if next(model.backbone.blocks[1].parameters()).requires_grad:\n",
    "                model.unfreeze_step(BLOCKS_PER_STEP)\n",
    "                print(f\"\\n VICTORY LAP started at Epoch {epoch} üèÜ\")\n",
    "                victory_lap_started = True\n",
    "\n",
    "                # Switch Optimizer to Low & Slow\n",
    "                optimizer = torch.optim.AdamW(\n",
    "                    [\n",
    "                        {\"params\": model.backbone.parameters(), \"lr\": p2_backbone_lr},\n",
    "                        {\"params\": model.shared.parameters(), \"lr\": p2_head_lr},\n",
    "                        {\"params\": model.reg_head.parameters(), \"lr\": p2_head_lr},\n",
    "                        {\"params\": model.cls_head.parameters(), \"lr\": p2_head_lr},\n",
    "                    ],\n",
    "                    weight_decay=0.02,\n",
    "                )\n",
    "\n",
    "                # Switch Scheduler to Plateau (Patience=3, Factor=0.5)\n",
    "                scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                    optimizer, mode=\"min\", factor=0.5, patience=3, verbose=True\n",
    "                )\n",
    "\n",
    "                # Tighten Early Stopping for the end game\n",
    "                patience_counter = 0\n",
    "\n",
    "            else:\n",
    "                patience_counter = 0  # reset patience on unfreeze\n",
    "                # rebind optimizer to capture new blocks that were unfrozen\n",
    "                num_unfrozen = epoch // UNFREEZE_INTERVAL\n",
    "\n",
    "                current_backbone_lr = p1_backbone_lr * 0.9**num_unfrozen\n",
    "                current_head_lr = p1_base_head_lr * 0.95**num_unfrozen\n",
    "\n",
    "                optimizer = torch.optim.AdamW(\n",
    "                    [\n",
    "                        {\"params\": model.backbone.parameters(), \"lr\": current_backbone_lr},\n",
    "                        {\"params\": model.shared.parameters(), \"lr\": current_head_lr},\n",
    "                        {\"params\": model.reg_head.parameters(), \"lr\": current_head_lr},\n",
    "                        {\"params\": model.cls_head.parameters(), \"lr\": current_head_lr},\n",
    "                    ],\n",
    "                    weight_decay=0.01,\n",
    "                )\n",
    "\n",
    "                # Restart scheduler with the new, lower ceiling\n",
    "                scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "                    optimizer, T_0=UNFREEZE_INTERVAL, T_mult=1, eta_min=1e-7\n",
    "                )\n",
    "                print(\"--- Optimizer & Scheduler Reset for New Backbone Blocks ---\")\n",
    "\n",
    "        # Training Phase\n",
    "        model.train()  # Dropout ON\n",
    "        train_running_loss = 0.0\n",
    "\n",
    "        pbar = tqdm(\n",
    "            train_loader,\n",
    "            desc=f\"Epoch {epoch + 1}/{epochs}\",\n",
    "            leave=False,\n",
    "            unit=\"batch\",\n",
    "            mininterval=0.5,\n",
    "        )\n",
    "\n",
    "        for batch_idx, (images, labels_coords, labels_zones) in enumerate(pbar):\n",
    "            # non_blocking=True speeds up RAM-to-VRAM transfer\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            labels_coords = labels_coords.to(device, non_blocking=True)\n",
    "            labels_zones = labels_zones.to(device, non_blocking=True)\n",
    "\n",
    "            if is_rtx:  # Optimize for RTX GPUs that prefer channels_last\n",
    "                images = images.to(memory_format=torch.channels_last)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            if is_rtx:  # Use Mixed Precision Training only on RTX cards\n",
    "                with autocast(\"cuda\", dtype=torch.float16):\n",
    "                    pred_coords, pred_zones = model(images)\n",
    "\n",
    "                    loss_reg = criterion_reg(pred_coords, labels_coords)\n",
    "                    loss_cls = criterion_cls(pred_zones, labels_zones)\n",
    "\n",
    "                    loss = loss_reg + (0.5 * loss_cls)\n",
    "\n",
    "                gradScaler.scale(loss).backward()\n",
    "                gradScaler.step(optimizer)\n",
    "                gradScaler.update()\n",
    "\n",
    "            else:  # Standard training for non-RTX cards\n",
    "                pred_coords, pred_zones = model(images)\n",
    "\n",
    "                loss_reg = criterion_reg(pred_coords, labels_coords)\n",
    "                loss_cls = criterion_cls(pred_zones, labels_zones)\n",
    "\n",
    "                loss = loss_reg + (0.5 * loss_cls)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Step the cosine annealing scheduler \n",
    "            if not victory_lap_started:\n",
    "                scheduler.step(epoch + batch_idx / len(train_loader))\n",
    "            train_running_loss += loss.item()\n",
    "\n",
    "        # Validation Phase\n",
    "        model.eval()  # Set model to evaluation mode (disables Dropout)\n",
    "        val_running_loss = 0.0\n",
    "        correct_zones = 0\n",
    "        raw_preds_coords = []\n",
    "        raw_trues_coords = []\n",
    "\n",
    "        with torch.no_grad(): \n",
    "            for images, labels_coords, labels_zones in val_loader:\n",
    "                images = images.to(device, non_blocking=True)\n",
    "                labels_coords = labels_coords.to(device, non_blocking=True)\n",
    "                labels_zones = labels_zones.to(device, non_blocking=True)\n",
    "\n",
    "                # Standard Prediction and Loss Calculation\n",
    "                pred_coords, pred_zones = model(images)\n",
    "                loss_reg = criterion_reg(pred_coords, labels_coords)\n",
    "                loss_cls = criterion_cls(pred_zones, labels_zones)\n",
    "                val_running_loss += (loss_reg + 0.5 * loss_cls).item()\n",
    "\n",
    "                raw_preds_coords.append(pred_coords.cpu().numpy())\n",
    "                raw_trues_coords.append(labels_coords.cpu().numpy())\n",
    "\n",
    "                predicted_zones = torch.argmax(pred_zones, dim=1)\n",
    "                correct_zones += (predicted_zones == labels_zones).sum().item()\n",
    "\n",
    "        # Concatenate all batches, convert to real-world coordinates\n",
    "        full_preds_raw = np.vstack(raw_preds_coords)\n",
    "        full_trues_raw = np.vstack(raw_trues_coords)\n",
    "        real_preds = scaler.inverse_transform(full_preds_raw)\n",
    "        real_trues = scaler.inverse_transform(full_trues_raw)\n",
    "\n",
    "        # Calculate distance errors\n",
    "        distances = haversine_distance(real_preds, real_trues)\n",
    "        avg_dist_error = np.mean(distances)\n",
    "\n",
    "        if victory_lap_started:# ReduceLROnPlateau step\n",
    "            scheduler.step(avg_dist_error)  \n",
    "\n",
    "        median_dist_error = np.median(distances)\n",
    "        zone_accuracy = correct_zones / len(val_dataset) * 100.0\n",
    "        avg_train_loss = train_running_loss / len(train_loader)\n",
    "        avg_val_loss = val_running_loss / len(val_loader)\n",
    "\n",
    "        if victory_lap_started:\n",
    "            current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "        else:\n",
    "            current_lr = scheduler.get_last_lr()[0]\n",
    "\n",
    "        if use_TensorBoard:  # Write to TensorBoard\n",
    "            writer_train.add_scalar(\"MSE Loss\", avg_train_loss, epoch)\n",
    "            writer_val.add_scalar(\"MSE Loss\", avg_val_loss, epoch)\n",
    "            writer_val.add_scalar(\n",
    "                \"Metrics/Avg_distance_Error_Meters\", avg_dist_error, epoch\n",
    "            )\n",
    "            writer_val.add_scalar(\n",
    "                \"Metrics/Median_distance_Error_Meters\", median_dist_error, epoch\n",
    "            )\n",
    "            writer_val.add_scalar(\"Metrics/Zone_Accuracy_Percent\", zone_accuracy, epoch)\n",
    "            writer_train.add_scalar(\"Hyperparameters/Learning_Rate\", current_lr, epoch)\n",
    "\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        val_avg_dist_history.append(avg_dist_error)\n",
    "        val_median_dist_history.append(median_dist_error)\n",
    "        val_zone_accuracy.append(zone_accuracy)\n",
    "        learning_rates.append(current_lr)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}: Train Loss {avg_train_loss:.4f} | Val Loss {avg_val_loss:.4f} | \"\n",
    "            f\"Avg Dist Error {avg_dist_error:.1f}m | Median Dist Error {median_dist_error:.1f}m | Zone Acc {zone_accuracy:.1f}%\"\n",
    "        )\n",
    "\n",
    "        # Save the best version of the model\n",
    "        if avg_dist_error < best_dist:\n",
    "            best_dist = avg_dist_error\n",
    "            best_epoch = epoch + 1\n",
    "\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            patience_counter = 0\n",
    "            print(f\"  *** NEW BEST: {best_dist:.1f}m ***\")\n",
    "\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"(No improvement for {patience_counter}/{early_stopping_patience} epochs)\")\n",
    "\n",
    "            # Early Stopping check\n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                print(\"Model stopped improving. Ending training early.\")\n",
    "                break\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n\\nTraining Interrupted by User. Saving current progress...\")\n",
    "\n",
    "# Rename the best model file to a descriptive name\n",
    "if os.path.exists(best_model_path):\n",
    "    current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    final_name = f\"trained_models/geo_model_epoch_{best_epoch}_mean_{best_dist:.2f}_{current_time}.pth\"\n",
    "            \n",
    "    # Rename the temp file\n",
    "    os.rename(best_model_path, final_name)\n",
    "    print(f\"\\n Best model saved to: {final_name}\")\n",
    "\n",
    "if use_TensorBoard:\n",
    "    writer_train.close()\n",
    "    writer_val.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv (3.13.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
