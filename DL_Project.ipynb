{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "84b4d245",
      "metadata": {},
      "source": [
        "# Imports:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fee74d4",
      "metadata": {
        "id": "1fee74d4"
      },
      "outputs": [],
      "source": [
        "# Makes sure to reload modules when they change\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "# --- Standard Library Imports ---\n",
        "import os\n",
        "import joblib\n",
        "from PIL import Image, ImageOps\n",
        "\n",
        "# --- Third-Party Imports ---\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.amp import GradScaler, autocast\n",
        "import torch\n",
        "\n",
        "# --- Custom Imports ---\n",
        "from src.utils import (\n",
        "    extractCoordinates, aspect_crop, haversine_distance,\n",
        "    plot_images_from_dataloader, setup_TensorBoard_writers,\n",
        "    log_error_map, create_interactive_heatmap\n",
        ")\n",
        "from src.dataset import GeolocalizationDataset\n",
        "from src.models import ConvNet, ConvNet2, ConvNet3, MultiTaskDINOGeo\n",
        "\n",
        "# Enable CUDA optimizations\n",
        "torch.backends.cudnn.benchmark = True"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bc16e7b",
      "metadata": {},
      "source": [
        "# Image preprocessing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90122299",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup paths\n",
        "RAW_IMAGE_FOLDER = r\"data_manual_gps_united\"             # Use original images for GPS extraction\n",
        "PROCESSED_IMAGE_FOLDER = r\"data_processed_manual_gps\" # Use processed images for training\n",
        "\n",
        "if not os.path.exists(PROCESSED_IMAGE_FOLDER) or len(os.listdir(PROCESSED_IMAGE_FOLDER)) < 1475:\n",
        "    os.makedirs(PROCESSED_IMAGE_FOLDER, exist_ok=True)\n",
        "\n",
        "    print(\"Starting Pre-processing...\")\n",
        "    files = [f for f in os.listdir(RAW_IMAGE_FOLDER) if f.lower().endswith(('.jpg', '.jpeg'))]\n",
        "\n",
        "    for filename in tqdm(files):\n",
        "        src_path = os.path.join(RAW_IMAGE_FOLDER, filename)\n",
        "        dst_path = os.path.join(PROCESSED_IMAGE_FOLDER, filename)\n",
        "        \n",
        "        try:\n",
        "            with Image.open(src_path) as img:\n",
        "                img = ImageOps.exif_transpose(img)\n",
        "                img = img.convert('RGB')\n",
        "                img = aspect_crop(img) \n",
        "                img = img.resize((192, 256), Image.Resampling.LANCZOS)\n",
        "                img.save(dst_path, quality=95)\n",
        "        except Exception as e:\n",
        "            print(f\"Failed {filename}: {e}\")\n",
        "else:\n",
        "    print(\"Pre-processed images already exist. Skipping preprocessing step.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "979e38b5",
      "metadata": {},
      "source": [
        "# Data loading:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25b3188d",
      "metadata": {},
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    SCALER_SAVE_PATH = 'coordinate_scaler.pkl'\n",
        "\n",
        "    # --- 2. EXTRACTION PHASE ---\n",
        "    processed_data = []\n",
        "\n",
        "    for filename in os.listdir(RAW_IMAGE_FOLDER):\n",
        "        if filename.lower().endswith(('.jpg', '.jpeg')):\n",
        "            raw_image_path = os.path.join(RAW_IMAGE_FOLDER, filename)\n",
        "            processed__image_path = os.path.join(PROCESSED_IMAGE_FOLDER, filename)\n",
        "            \n",
        "            # Check if the processed version actually exists\n",
        "            if not os.path.exists(processed__image_path):\n",
        "                continue\n",
        "                \n",
        "            # Extract coordinates from the ORIGINAL file\n",
        "            coords = extractCoordinates(raw_image_path)\n",
        "            \n",
        "            if coords:\n",
        "                processed_data.append({\n",
        "                    'path': processed__image_path, \n",
        "                    'lat': coords[0], \n",
        "                    'lon': coords[1]\n",
        "                })\n",
        "\n",
        "    df = pd.DataFrame(processed_data)\n",
        "    \n",
        "    # Keep 20% of the data for validation\n",
        "    train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "    \n",
        "    # --- 3. SCALING PHASE ---\n",
        "    # must fit on training data *only*\n",
        "    scaler = MinMaxScaler()\n",
        "    train_df[['lat', 'lon']] = scaler.fit_transform(train_df[['lat', 'lon']])\n",
        "    val_df[['lat', 'lon']] = scaler.transform(val_df[['lat', 'lon']])\n",
        "\n",
        "    joblib.dump(scaler, SCALER_SAVE_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf405cb2",
      "metadata": {},
      "source": [
        "## Clustering for multi-task training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "548cacad",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "print(\"Generating Zone Labels on Normalized Data...\")\n",
        "\n",
        "NUM_ZONES = 25 \n",
        "\n",
        "kmeans = KMeans(n_clusters=NUM_ZONES, random_state=42, n_init=10)\n",
        "\n",
        "# Fit on the normalized coordinates\n",
        "train_df['zone_label'] = kmeans.fit_predict(train_df[['lat', 'lon']])\n",
        "val_df['zone_label'] = kmeans.predict(val_df[['lat', 'lon']])\n",
        "\n",
        "# plot the zones to see if they look reasonable for both training and validation\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Training data plot\n",
        "axes[0].scatter(train_df['lon'], train_df['lat'], c=train_df['zone_label'], cmap='tab20', s=30)\n",
        "axes[0].set_title(f\"Training Data Divided into {NUM_ZONES} Zones\")\n",
        "axes[0].axis('equal')  # Keep aspect ratio so it looks like a map\n",
        "\n",
        "# Validation data plot\n",
        "axes[1].scatter(val_df['lon'], val_df['lat'], c=val_df['zone_label'], cmap='tab20', s=30)\n",
        "axes[1].set_title(f\"Validation Data Divided into {NUM_ZONES} Zones\")\n",
        "axes[1].axis('equal')  # Keep aspect ratio so it looks like a map\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26fd8cab",
      "metadata": {},
      "source": [
        "# Plot heatmap based on training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ced5d314",
      "metadata": {},
      "outputs": [],
      "source": [
        "train_coords = scaler.inverse_transform(train_df[['lat', 'lon']].values)\n",
        "create_interactive_heatmap(train_coords, output_file='train_data_heatmap.html')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7MXnRC5rRo8v",
      "metadata": {
        "id": "7MXnRC5rRo8v"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # --- 4. DATASET INITIALIZATION ---\n",
        "    print(\"Initializing train dataset...\")\n",
        "    train_dataset = GeolocalizationDataset(\n",
        "        image_paths=train_df['path'].tolist(),\n",
        "        coordinates=train_df[['lat', 'lon']].values,\n",
        "        zone_labels=train_df['zone_label'].values,\n",
        "        is_train=True,\n",
        "        target_size=(252, 182)\n",
        "    )\n",
        "    print(\"Initializing validation dataset...\")\n",
        "    val_dataset = GeolocalizationDataset(\n",
        "        image_paths=val_df['path'].tolist(),\n",
        "        coordinates=val_df[['lat', 'lon']].values,\n",
        "        zone_labels=val_df['zone_label'].values,\n",
        "        is_train=False,\n",
        "        target_size=(252, 182)\n",
        "    )\n",
        "\n",
        "    # --- 5. THE DATALOADER ---\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=64,\n",
        "        shuffle=True,\n",
        "        num_workers=0, \n",
        "        pin_memory=True)\n",
        "    \n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=128,\n",
        "        shuffle=False,\n",
        "        num_workers=0, \n",
        "        pin_memory=False)\n",
        "    \n",
        "    plot_images_from_dataloader(train_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e14bbed1",
      "metadata": {},
      "source": [
        "# Model setup:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bb34c98",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 6. INITIALIZE MODEL ---\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "is_rtx = \"RTX\" in torch.cuda.get_device_name(0)\n",
        "print(\"Using device:\", device)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "\n",
        "model = MultiTaskDINOGeo(NUM_ZONES).to(device)\n",
        "\n",
        "if is_rtx:\n",
        "    model = model.to(\n",
        "        memory_format=torch.channels_last\n",
        "    )  # Optimize for modern GPUs that prefer channels_last\n",
        "else:\n",
        "    print(\"RTX card not detected: Disabling AMP/Channels_Last optimizations\")\n",
        "    model = model.to(device)\n",
        "\n",
        "# --- 7. LOSS & OPTIMIZER ---\n",
        "UNFREEZE_INTERVAL = 20  # epochs between unfreezing backbone blocks\n",
        "BLOCKS_PER_STEP = 1     # How many blocks to open at once\n",
        "victory_lap_started = False\n",
        "\n",
        "patience_counter = 0\n",
        "early_stopping_patience = UNFREEZE_INTERVAL - 1 # stop early if no improvement before next unfreeze\n",
        "epochs = 500\n",
        "use_TensorBoard = True  # Set to False to disable TensorBoard logging\n",
        "\n",
        "criterion_reg = torch.nn.HuberLoss(delta=1.0)\n",
        "criterion_cls = torch.nn.CrossEntropyLoss(label_smoothing=0.15)\n",
        "\n",
        "# Learning rates for phase 1 - iterative unfreezing\n",
        "p1_base_head_lr = 1e-3\n",
        "p1_backbone_lr = 5e-5\n",
        "\n",
        "# Learning rates for phase 2 - fine-tuning all layers\n",
        "p2_backbone_lr = 1e-5  \n",
        "p2_head_lr = 5e-4  \n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    [\n",
        "        {\n",
        "            \"params\": filter(lambda p: p.requires_grad, model.backbone.parameters()),\n",
        "            \"lr\": p1_backbone_lr,\n",
        "        },\n",
        "        {\"params\": model.shared.parameters(), \"lr\": p1_base_head_lr},\n",
        "        {\"params\": model.reg_head.parameters(), \"lr\": p1_base_head_lr},\n",
        "        {\"params\": model.cls_head.parameters(), \"lr\": p1_base_head_lr},\n",
        "    ],\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "    optimizer, T_0=UNFREEZE_INTERVAL, T_mult=1, eta_min=1e-7\n",
        ")\n",
        "\n",
        "print(\n",
        "    f\"Training on {len(train_dataset)} images, Validating on {len(val_dataset)} images.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83984718",
      "metadata": {},
      "source": [
        "# Model training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f99a6546",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 8. TRAINING & VALIDATION LOOP ---\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "val_avg_dist_history = []\n",
        "val_median_dist_history = []\n",
        "val_zone_accuracy = []\n",
        "learning_rates = []\n",
        "best_dist = float(\"inf\")\n",
        "\n",
        "if use_TensorBoard:\n",
        "    writer_train, writer_val = setup_TensorBoard_writers()\n",
        "\n",
        "print(f\"Starting training on {device}...\")\n",
        "\n",
        "gradScaler = GradScaler(\n",
        "    \"cuda\"\n",
        ")  # scaler for mixed precision training, prevents gradient underflow\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Check for Unfreezing layer blocks\n",
        "    if epoch > 0 and epoch % UNFREEZE_INTERVAL == 0 and not victory_lap_started:\n",
        "\n",
        "        model.unfreeze_step(BLOCKS_PER_STEP)\n",
        "\n",
        "        # Check if all blocks are unfrozen - phase 2 begins\n",
        "        if next(model.backbone.blocks[0].parameters()).requires_grad: \n",
        "            print(f\"\\nüèÜ VICTORY LAP DETECTED at Epoch {epoch} üèÜ\")\n",
        "            victory_lap_started = True\n",
        "            \n",
        "            # Switch Optimizer to Low & Slow\n",
        "            optimizer = torch.optim.AdamW([\n",
        "                {'params': model.backbone.parameters(), 'lr': p2_backbone_lr},\n",
        "                {'params': model.shared.parameters(),   'lr': p2_head_lr},\n",
        "                {'params': model.reg_head.parameters(), 'lr': p2_head_lr},\n",
        "                {'params': model.cls_head.parameters(), 'lr': p2_head_lr}\n",
        "            ], weight_decay=0.02)\n",
        "            \n",
        "            # Switch Scheduler to Plateau (Patience=3, Factor=0.5)\n",
        "            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "                optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
        "            )\n",
        "            \n",
        "            # Tighten Early Stopping for the end game\n",
        "            patience_counter = 0\n",
        "\n",
        "        else:\n",
        "            patience_counter = 0  # reset patience on unfreeze\n",
        "            # rebind optimizer to capture new blocks that were unfrozen\n",
        "            num_unfrozen = epoch // UNFREEZE_INTERVAL\n",
        "\n",
        "            current_backbone_lr = p1_backbone_lr * 0.9**num_unfrozen\n",
        "            current_head_lr = p1_base_head_lr * 0.95**num_unfrozen\n",
        "\n",
        "            optimizer = torch.optim.AdamW(\n",
        "                [\n",
        "                    {\"params\": model.backbone.parameters(), \"lr\": current_backbone_lr},\n",
        "                    {\"params\": model.shared.parameters(), \"lr\": current_head_lr},\n",
        "                    {\"params\": model.reg_head.parameters(), \"lr\": current_head_lr},\n",
        "                    {\"params\": model.cls_head.parameters(), \"lr\": current_head_lr},\n",
        "                ],\n",
        "                weight_decay=0.01,\n",
        "            )\n",
        "\n",
        "            # Restart scheduler with the new, lower ceiling\n",
        "            scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "                optimizer, T_0=UNFREEZE_INTERVAL, T_mult=1, eta_min=1e-7\n",
        "            )\n",
        "            print(\"--- Optimizer & Scheduler Reset for New Backbone Blocks ---\")\n",
        "\n",
        "    # --- PHASE 1: TRAINING ---\n",
        "    model.train()  # Dropout ON\n",
        "    train_running_loss = 0.0\n",
        "\n",
        "    pbar = tqdm(\n",
        "        train_loader,\n",
        "        desc=f\"Epoch {epoch+1}/{epochs}\",\n",
        "        leave=False,\n",
        "        unit=\"batch\",\n",
        "        mininterval=0.5,\n",
        "    )\n",
        "\n",
        "    for batch_idx, (images, labels_coords, labels_zones) in enumerate(pbar):\n",
        "        # non_blocking=True speeds up RAM-to-VRAM transfer\n",
        "        images = images.to(device, non_blocking=True)\n",
        "        labels_coords = labels_coords.to(device, non_blocking=True)\n",
        "        labels_zones = labels_zones.to(device, non_blocking=True)\n",
        "\n",
        "        if is_rtx:  # Optimize for RTX GPUs that prefer channels_last\n",
        "            images = images.to(memory_format=torch.channels_last)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        if is_rtx:  # Use Mixed Precision Training only on RTX cards\n",
        "            with autocast(\"cuda\", dtype=torch.float16):\n",
        "                pred_coords, pred_zones = model(images)\n",
        "\n",
        "                loss_reg = criterion_reg(pred_coords, labels_coords)\n",
        "                loss_cls = criterion_cls(pred_zones, labels_zones)\n",
        "\n",
        "                loss = loss_reg + (0.5 * loss_cls)\n",
        "\n",
        "            gradScaler.scale(loss).backward()\n",
        "            gradScaler.step(optimizer)\n",
        "            gradScaler.update()\n",
        "\n",
        "        else:  # Standard training for non-RTX cards\n",
        "            pred_coords, pred_zones = model(images)\n",
        "\n",
        "            loss_reg = criterion_reg(pred_coords, labels_coords)\n",
        "            loss_cls = criterion_cls(pred_zones, labels_zones)\n",
        "\n",
        "            loss = loss_reg + (0.5 * loss_cls)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        if not victory_lap_started:\n",
        "            scheduler.step(epoch + batch_idx / len(train_loader))\n",
        "        train_running_loss += loss.item()\n",
        "\n",
        "    # --- PHASE 2: VALIDATION ---\n",
        "    model.eval()  # Set model to evaluation mode (disables Dropout)\n",
        "    val_running_loss = 0.0\n",
        "    correct_zones = 0\n",
        "    raw_preds_coords = []\n",
        "    raw_trues_coords = []\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculation for efficiency\n",
        "        for images, labels_coords, labels_zones in val_loader:\n",
        "            # todo: channels last?\n",
        "            images = images.to(device, non_blocking=True)\n",
        "            labels_coords = labels_coords.to(device, non_blocking=True)\n",
        "            labels_zones = labels_zones.to(device, non_blocking=True)\n",
        "\n",
        "            # Standard Prediction and MSE Loss\n",
        "            pred_coords, pred_zones = model(images)\n",
        "            loss_reg = criterion_reg(pred_coords, labels_coords)\n",
        "            loss_cls = criterion_cls(pred_zones, labels_zones)\n",
        "            val_running_loss += (loss_reg + 0.5 * loss_cls).item()\n",
        "\n",
        "            raw_preds_coords.append(pred_coords.cpu().numpy())\n",
        "            raw_trues_coords.append(labels_coords.cpu().numpy())\n",
        "\n",
        "            predicted_zones = torch.argmax(pred_zones, dim=1)\n",
        "            correct_zones += (predicted_zones == labels_zones).sum().item()\n",
        "\n",
        "    full_preds_raw = np.vstack(raw_preds_coords)\n",
        "    full_trues_raw = np.vstack(raw_trues_coords)\n",
        "    real_preds = scaler.inverse_transform(full_preds_raw)\n",
        "    real_trues = scaler.inverse_transform(full_trues_raw)\n",
        "\n",
        "    # --- PHASE 3: METRICS CALCULATION & PRINTING ---\n",
        "    distances = haversine_distance(real_preds, real_trues)\n",
        "    avg_dist_error = np.mean(distances)\n",
        "\n",
        "    if victory_lap_started:\n",
        "        scheduler.step(avg_dist_error)  # ReduceLROnPlateau step\n",
        "        \n",
        "    median_dist_error = np.median(distances)\n",
        "    zone_accuracy = correct_zones / len(val_dataset) * 100.0\n",
        "\n",
        "    avg_train_loss = train_running_loss / len(train_loader)\n",
        "    avg_val_loss = val_running_loss / len(val_loader)\n",
        "    if victory_lap_started:\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "    else:\n",
        "        current_lr = scheduler.get_last_lr()[0]\n",
        "\n",
        "    if epoch % UNFREEZE_INTERVAL == 0:  # log error map every 15 epochs\n",
        "        log_error_map(\n",
        "            real_preds,\n",
        "            real_trues,\n",
        "            epoch,\n",
        "            TB_writer=writer_val if use_TensorBoard else None,\n",
        "        )\n",
        "\n",
        "    if use_TensorBoard:  # Write to TensorBoard\n",
        "        writer_train.add_scalar(\"MSE Loss\", avg_train_loss, epoch)\n",
        "        writer_val.add_scalar(\"MSE Loss\", avg_val_loss, epoch)\n",
        "        writer_val.add_scalar(\n",
        "            \"Metrics/Avg_distance_Error_Meters\", avg_dist_error, epoch\n",
        "        )\n",
        "        writer_val.add_scalar(\n",
        "            \"Metrics/Median_distance_Error_Meters\", median_dist_error, epoch\n",
        "        )\n",
        "        writer_val.add_scalar(\"Metrics/Zone_Accuracy_Percent\", zone_accuracy, epoch)\n",
        "        writer_train.add_scalar(\"Hyperparameters/Learning_Rate\", current_lr, epoch)\n",
        "\n",
        "    else:  # No TensorBoard: store in lists\n",
        "        train_losses.append(avg_train_loss)\n",
        "        val_losses.append(avg_val_loss)\n",
        "        val_avg_dist_history.append(avg_dist_error)\n",
        "        val_median_dist_history.append(median_dist_error)\n",
        "        val_zone_accuracy.append(zone_accuracy)\n",
        "        learning_rates.append(current_lr)\n",
        "\n",
        "    print(\n",
        "        f\"Epoch {epoch+1}: Train Loss {avg_train_loss:.4f} | Val Loss {avg_val_loss:.4f} | \"\n",
        "        f\"Avg Dist Error {avg_dist_error:.1f}m | Median Dist Error {median_dist_error:.1f}m | Zone Acc {zone_accuracy:.1f}%\"\n",
        "    )\n",
        "\n",
        "    # 2. Save the BEST version of YOUR model\n",
        "    if avg_dist_error < best_dist:\n",
        "        best_dist = avg_dist_error\n",
        "        torch.save(model.state_dict(), \"custom_geo_model_best.pth\")\n",
        "        patience_counter = 0\n",
        "        print(f\"  *** NEW BEST: {best_dist:.1f}m ***\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        print(\n",
        "            f\"(No improvement for {patience_counter}/{early_stopping_patience} epochs)\"\n",
        "        )\n",
        "        # 3. Early Stopping check\n",
        "        if patience_counter >= early_stopping_patience:\n",
        "            print(\"Model stopped improving. Ending training early.\")\n",
        "            break\n",
        "\n",
        "log_error_map(\n",
        "    real_preds, real_trues, epoch, TB_writer=writer_val if use_TensorBoard else None\n",
        ")  # final log\n",
        "\n",
        "if use_TensorBoard:\n",
        "    writer_train.close()\n",
        "    writer_val.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd27c7fc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 1. LOAD MODEL ---\n",
        "print(\"Loading Best Checkpoint for Victory Lap...\")\n",
        "model = MultiTaskDINOGeo(NUM_ZONES).to(device)\n",
        "\n",
        "# Load weights (ensure map_location is correct)\n",
        "checkpoint = torch.load(\"dino_geo_model_best_mean_13.1.pth\", map_location=device)\n",
        "model.load_state_dict(checkpoint)\n",
        "\n",
        "# --- 2. FORCE UNFREEZE EVERYTHING ---\n",
        "print(\"üîì Unfreezing ENTIRE Model...\")\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# --- 3. VICTORY LAP CONFIGURATION ---\n",
        "# We use a Plateau scheduler because we want to keep the LR high \n",
        "# as long as we are improving, then drop it when we get stuck.\n",
        "\n",
        "# HEADS: 5e-4 (Active fine-tuning)\n",
        "# BACKBONE: 1e-5 (Very safe \"cruising speed\" to avoid breaking features)\n",
        "victory_optimizer = torch.optim.AdamW([\n",
        "    # Backbone: 2e-6 (Extremely slow polish)\n",
        "    {'params': model.backbone.parameters(), 'lr': 2e-6},\n",
        "    \n",
        "    # Heads: 1e-4 (Standard fine-tuning speed)\n",
        "    {'params': model.shared.parameters(),   'lr': 1e-4},\n",
        "    {'params': model.reg_head.parameters(), 'lr': 1e-4},\n",
        "    {'params': model.cls_head.parameters(), 'lr': 1e-4}\n",
        "], weight_decay=0.02)\n",
        "\n",
        "victory_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    victory_optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
        ")\n",
        "\n",
        "# --- 4. TRAINING LOOP ---\n",
        "print(\"üèÜ STARTING VICTORY LAP (Full Fine-Tuning) üèÜ\")\n",
        "patience_counter = 0\n",
        "early_stopping = 15 # Shorter patience since we are just polishing\n",
        "best_dist = 13.1    # Set this to your current best to avoid saving worse models!\n",
        "\n",
        "for epoch in range(50): # Run for ~50 epochs, usually enough\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    \n",
        "    # Training Step\n",
        "    for images, labels_coords, labels_zones in tqdm(train_loader, desc=f\"Victory Epoch {epoch+1}\"):\n",
        "        images = images.to(device)\n",
        "        labels_coords = labels_coords.to(device)\n",
        "        labels_zones = labels_zones.to(device)\n",
        "        \n",
        "        victory_optimizer.zero_grad()\n",
        "        \n",
        "        pred_coords, pred_zones = model(images)\n",
        "        loss_reg = criterion_reg(pred_coords, labels_coords)\n",
        "        loss_cls = criterion_cls(pred_zones, labels_zones)\n",
        "        loss = loss_reg + (0.5 * loss_cls)\n",
        "        \n",
        "        loss.backward()\n",
        "        victory_optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # Validation Step\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    raw_preds = []\n",
        "    raw_trues = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for images, labels_coords, labels_zones in val_loader:\n",
        "            images = images.to(device)\n",
        "            labels_coords = labels_coords.to(device)\n",
        "            labels_zones = labels_zones.to(device)\n",
        "            \n",
        "            pred_coords, pred_zones = model(images)\n",
        "            loss_reg = criterion_reg(pred_coords, labels_coords)\n",
        "            loss_cls = criterion_cls(pred_zones, labels_zones)\n",
        "            val_loss += (loss_reg + 0.5 * loss_cls).item()\n",
        "            \n",
        "            raw_preds.append(pred_coords.cpu().numpy())\n",
        "            raw_trues.append(labels_coords.cpu().numpy())\n",
        "\n",
        "    # Metrics\n",
        "    full_preds = scaler.inverse_transform(np.vstack(raw_preds))\n",
        "    full_trues = scaler.inverse_transform(np.vstack(raw_trues))\n",
        "    dist_errors = haversine_distance(full_preds, full_trues)\n",
        "    avg_dist = np.mean(dist_errors)\n",
        "    median_dist = np.median(dist_errors)\n",
        "    \n",
        "    # Scheduler Step (Watch Distance, not Loss)\n",
        "    victory_scheduler.step(avg_dist)\n",
        "    \n",
        "    print(f\"Epoch {epoch+1}: Avg Dist: {avg_dist:.1f}m | Median: {median_dist:.1f}m\")\n",
        "    \n",
        "    # Save if improved\n",
        "    if avg_dist < best_dist:\n",
        "        best_dist = avg_dist\n",
        "        torch.save(model.state_dict(), \"custom_geo_model_victory.pth\")\n",
        "        print(f\"‚ú® NEW BEST: {best_dist:.2f}m ‚ú®\")\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= early_stopping:\n",
        "            print(\"Victory Lap Finished.\")\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "964009f1",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "def evaluate_with_knn(model, train_loader, val_loader, k=3):\n",
        "    model.eval()\n",
        "    print(\"Building Feature Bank from Training Data...\")\n",
        "    \n",
        "    # 1. Extract features for all TRAINING images\n",
        "    X_train = []\n",
        "    y_train = []\n",
        "    with torch.no_grad():\n",
        "        for images, coords, _ in tqdm(train_loader):\n",
        "            images = images.to(device)\n",
        "            # Use the backbone + shared layer (the \"Smart\" features)\n",
        "            # We bypass the heads entirely\n",
        "            features = model.shared(model.backbone(images))\n",
        "            X_train.append(features.cpu().numpy())\n",
        "            y_train.append(coords.numpy()) # Keep normalized coords\n",
        "            \n",
        "    X_train = np.vstack(X_train)\n",
        "    y_train = np.vstack(y_train)\n",
        "    \n",
        "    # 2. Extract features for VALIDATION images\n",
        "    print(\"Querying Validation Data...\")\n",
        "    X_val = []\n",
        "    y_val = []\n",
        "    with torch.no_grad():\n",
        "        for images, coords, _ in tqdm(val_loader):\n",
        "            images = images.to(device)\n",
        "            features = model.shared(model.backbone(images))\n",
        "            X_val.append(features.cpu().numpy())\n",
        "            y_val.append(coords.numpy())\n",
        "            \n",
        "    X_val = np.vstack(X_val)\n",
        "    y_val = np.vstack(y_val)\n",
        "\n",
        "    # 3. Run k-NN (Cosine distance is usually best for DINO)\n",
        "    knn = KNeighborsRegressor(n_neighbors=k, metric='cosine', weights='distance')\n",
        "    knn.fit(X_train, y_train)\n",
        "    preds = knn.predict(X_val)\n",
        "    \n",
        "    # 4. Calculate Real Error\n",
        "    real_preds = scaler.inverse_transform(preds)\n",
        "    real_trues = scaler.inverse_transform(y_val)\n",
        "    distances = haversine_distance(real_preds, real_trues)\n",
        "    \n",
        "    print(f\"=== k-NN Results (k={k}) ===\")\n",
        "    print(f\"Mean Error: {np.mean(distances):.1f} meters, Median Error: {np.median(distances):.1f} meters\")\n",
        "\n",
        "model.load_state_dict(torch.load(\"custom_geo_model_best.pth\", map_location=device))\n",
        "for i in range(1, 10):\n",
        "    evaluate_with_knn(model, train_loader, val_loader, k=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "853e8950",
      "metadata": {},
      "source": [
        "# Plotting the results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97a601f0",
      "metadata": {},
      "outputs": [],
      "source": [
        "log_error_map(real_preds, real_trues, epoch,num_points= 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17b61f5f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 9. SAVE THE MODEL & GENERATE PLOTS ---\n",
        "\n",
        "# Create a figure with two subplots (1 row, 2 columns)\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Left Plot: MSE Loss (Training vs Validation)\n",
        "ax1.plot(train_losses, label=\"Training Loss\", color=\"blue\", linewidth=2)\n",
        "ax1.plot(val_losses, label=\"Validation Loss\", color=\"red\", linestyle=\"--\", linewidth=2)\n",
        "ax1.set_title(\"Mathematical Performance (MSE Loss)\")\n",
        "ax1.set_xlabel(\"Epoch\")\n",
        "ax1.set_ylabel(\"MSE Loss\")\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Right Plot: Physical Distance Error (Meters)\n",
        "ax2.plot(val_dist_history, label=\"Avg Distance Error\", color=\"green\", linewidth=2)\n",
        "ax2.set_title(\"Real-World Performance (Distance Error)\")\n",
        "ax2.set_xlabel(\"Epoch\")\n",
        "ax2.set_ylabel(\"Error (Meters)\")\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Save the model weights\n",
        "torch.save(model.state_dict(), \"geo_model.pth\")\n",
        "print(f\"\\nTraining finished! Best Validation Error: {best_dist:.1f} meters.\")\n",
        "print(\"Model saved as 'geo_model.pth'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ee3d9fa",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\" # Check if GPU is available, otherwise use CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "SCALER_PATH = 'coordinate_scaler.pkl'\n",
        "MODEL_WEIGHTS_PATH = 'geo_model.pth'\n",
        "\n",
        "# Loading Model and Scaler\n",
        "# Initialize the model architecture and move to the device (GPU/CPU)\n",
        "model = ConvNet2().to(device)\n",
        "\n",
        "# Load the trained weights from the .pth file\n",
        "model.load_state_dict(torch.load(MODEL_WEIGHTS_PATH))\n",
        "\n",
        "# Set the model to evaluation mode (disables Dropout and Batchnorm layers)\n",
        "model.eval()\n",
        "\n",
        "# Load the MinMaxScaler used during training to reverse the normalization\n",
        "scaler = joblib.load(SCALER_PATH)\n",
        "\n",
        "# Image Preprocessing Function\n",
        "def predict_location(image_path):\n",
        "\n",
        "    # Load the image and ensure it is in RGB format\n",
        "    img = Image.open(image_path).convert('RGB')\n",
        "    \n",
        "    # Apply the same validation transforms (No augmentations here!)\n",
        "    preprocess = T.Compose([\n",
        "        T.Resize(256),\n",
        "        T.CenterCrop(256),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    \n",
        "    # Add a batch dimension (Batch size of 1) and move the tensor to device\n",
        "    img_tensor = preprocess(img).unsqueeze(0).to(device) \n",
        "    \n",
        "    # Perform inference without calculating gradients\n",
        "    with torch.no_grad():\n",
        "        output = model(img_tensor)\n",
        "    \n",
        "    # Convert the prediction back to a NumPy array on the CPU\n",
        "    prediction_normalized = output.cpu().numpy()\n",
        "    \n",
        "    # Reverse the scaling to get real-world GPS coordinates\n",
        "    real_coords = scaler.inverse_transform(prediction_normalized)\n",
        "    \n",
        "    # Return the first (and only) result in the batch [Latitude, Longitude]\n",
        "    return real_coords[0]\n",
        "\n",
        "# Run Inference on a New Image\n",
        "# Provide the full path to your local image file\n",
        "test_path = r\"C:\\path\\to\\your\\new\\image.jpg\"\n",
        "lat, lon = predict_location(test_path)\n",
        "\n",
        "print(f\"Predicted Location: Latitude {lat:.6f}, Longitude {lon:.6f}\")\n",
        "print(f\"Google Maps Link: http://maps.google.com/maps?q={lat},{lon}\") \"\"\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv (3.13.0)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
