{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "84b4d245",
      "metadata": {},
      "source": [
        "# Imports:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "687cd1d7",
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "from PIL import Image, ImageOps\n",
        "\n",
        "# --- CUSTOM IMPORTS ---\n",
        "from src.utils import extractCoordinates, aspect_crop, haversine_distance, plot_images_from_dataloader, setup_TensorBoard_writers, log_error_map\n",
        "from src.dataset import GeolocalizationDataset\n",
        "from src.models import ConvNet, ConvNet2, ConvNet3, HierarchicalLocalizer\n",
        "\n",
        "torch.backends.cudnn.benchmark = True"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bc16e7b",
      "metadata": {},
      "source": [
        "# Image preprocessing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90122299",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup paths\n",
        "RAW_IMAGE_FOLDER = r\"data\"             # Use original images for GPS extraction\n",
        "PROCESSED_IMAGE_FOLDER = r\"data_processed\" # Use processed images for training\n",
        "\n",
        "if not os.path.exists(PROCESSED_IMAGE_FOLDER) or len(os.listdir(PROCESSED_IMAGE_FOLDER)) < 1479:\n",
        "    os.makedirs(PROCESSED_IMAGE_FOLDER, exist_ok=True)\n",
        "\n",
        "    print(\"Starting Pre-processing...\")\n",
        "    files = [f for f in os.listdir(RAW_IMAGE_FOLDER) if f.lower().endswith(('.jpg', '.jpeg'))]\n",
        "\n",
        "    for filename in tqdm(files):\n",
        "        src_path = os.path.join(RAW_IMAGE_FOLDER, filename)\n",
        "        dst_path = os.path.join(PROCESSED_IMAGE_FOLDER, filename)\n",
        "        \n",
        "        try:\n",
        "            with Image.open(src_path) as img:\n",
        "                img = ImageOps.exif_transpose(img)\n",
        "                img = img.convert('RGB')\n",
        "                img = aspect_crop(img) \n",
        "                img = img.resize((192, 256), Image.Resampling.LANCZOS)\n",
        "                img.save(dst_path, quality=95)\n",
        "        except Exception as e:\n",
        "            print(f\"Failed {filename}: {e}\")\n",
        "else:\n",
        "    print(\"Pre-processed images already exist. Skipping preprocessing step.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "979e38b5",
      "metadata": {},
      "source": [
        "# Data loading:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25b3188d",
      "metadata": {},
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    SCALER_SAVE_PATH = 'coordinate_scaler.pkl'\n",
        "\n",
        "    # --- 2. EXTRACTION PHASE ---\n",
        "    processed_data = []\n",
        "\n",
        "    for filename in os.listdir(RAW_IMAGE_FOLDER):\n",
        "        if filename.lower().endswith(('.jpg', '.jpeg')):\n",
        "            raw_image_path = os.path.join(RAW_IMAGE_FOLDER, filename)\n",
        "            processed__image_path = os.path.join(PROCESSED_IMAGE_FOLDER, filename)\n",
        "            \n",
        "            # Check if the processed version actually exists\n",
        "            if not os.path.exists(processed__image_path):\n",
        "                continue\n",
        "                \n",
        "            # Extract coordinates from the ORIGINAL file\n",
        "            coords = extractCoordinates(raw_image_path)\n",
        "            \n",
        "            if coords:\n",
        "                processed_data.append({\n",
        "                    'path': processed__image_path, \n",
        "                    'lat': coords[0], \n",
        "                    'lon': coords[1]\n",
        "                })\n",
        "\n",
        "    images_df = pd.DataFrame(processed_data)\n",
        "    \n",
        "    # Keep 20% of the data for validation\n",
        "    train_df, val_df = train_test_split(images_df, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7MXnRC5rRo8v",
      "metadata": {
        "id": "7MXnRC5rRo8v"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # --- 4. DATASET INITIALIZATION ---\n",
        "    print(\"Initializing train dataset...\")\n",
        "    train_dataset = GeolocalizationDataset(\n",
        "        image_paths=train_df['path'].tolist(),\n",
        "        coordinates=train_df[['lat', 'lon']].values,\n",
        "        target_size=(182, 252),\n",
        "        is_train=False # Set to True if you want to apply data augmentations\n",
        "    )\n",
        "    print(\"Initializing validation dataset...\")\n",
        "    val_dataset = GeolocalizationDataset(\n",
        "        image_paths=val_df['path'].tolist(),\n",
        "        coordinates=val_df[['lat', 'lon']].values,\n",
        "        target_size=(182, 252),\n",
        "        is_train=False\n",
        "    )\n",
        "\n",
        "    # --- 5. THE DATALOADER ---\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=32,\n",
        "        shuffle=True,\n",
        "        num_workers=0, \n",
        "        pin_memory=True)\n",
        "    \n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=32,\n",
        "        shuffle=False,\n",
        "        num_workers=0, \n",
        "        pin_memory=False)\n",
        "    \n",
        "    plot_images_from_dataloader(train_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e14bbed1",
      "metadata": {},
      "source": [
        "# Model setup:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2b5d5c9",
      "metadata": {},
      "outputs": [],
      "source": [
        "localizer = HierarchicalLocalizer(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc62afbf",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n--- Running Evaluation ---\")\n",
        "\n",
        "def haversine_distance1(coord1, coord2):\n",
        "    R = 6371000.0\n",
        "    lat1, lon1 = np.radians(coord1)\n",
        "    lat2, lon2 = np.radians(coord2)\n",
        "    dlat, dlon = lat2 - lat1, lon2 - lon1\n",
        "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
        "    return 2 * R * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
        "\n",
        "\n",
        "total_error = 0.0\n",
        "errors = []\n",
        "\n",
        "# Create the tqdm object explicitly\n",
        "pbar = tqdm(range(len(val_dataset)), desc=\"Validating\")\n",
        "\n",
        "for i in pbar:\n",
        "    img, actual_gps = val_dataset[i]\n",
        "    \n",
        "    # Predict\n",
        "    pred_gps = localizer.predict(img,top_n_matches=10, MIN_INLIER_THRESHOLD=100, debug=True)\n",
        "    \n",
        "    # Measure\n",
        "    err = haversine_distance1(pred_gps, actual_gps.numpy())\n",
        "    errors.append(err)\n",
        "    \n",
        "    # # Optionally, display the matched image alongside the query image\n",
        "    # plt.figure(figsize=(8, 5))\n",
        "    # plt.subplot(1, 2, 1)\n",
        "    # plt.imshow(img.permute(1, 2, 0).cpu().numpy())\n",
        "    # plt.title(f'Query Image: {val_dataset.image_paths[i].split(os.sep)[-1]}')\n",
        "    # plt.axis('off')\n",
        "    # plt.subplot(1, 2, 2)\n",
        "    # plt.imshow(pred_img[0].permute(1, 2, 0).cpu().numpy())\n",
        "    # plt.title('Matched Image: {} \\n Error: {:.1f} m, Inlier Matches: {}'.format( val_dataset.image_paths[i].split(os.sep)[-1], err, inlier_matches))\n",
        "    # plt.axis('off')\n",
        "    # plt.show()\n",
        "\n",
        "    # Update running stats\n",
        "    total_error += err\n",
        "    avg_error = total_error / (i + 1)\n",
        "    \n",
        "    # Update the progress bar\n",
        "    # displaying current image error and the running average\n",
        "    pbar.set_postfix({'curr': f\"{err:.1f}m\", 'avg': f\"{avg_error:.1f}m\"})\n",
        "\n",
        "print(f\"Final Average Error: {np.mean(errors):.2f} meters\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6620dd84",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert errors to a numpy array for easier manipulation\n",
        "errors_array = np.array(errors)\n",
        "\n",
        "# Basic statistics\n",
        "print(f\"Mean Error: {np.mean(errors_array):.2f} meters\")\n",
        "print(f\"Median Error: {np.median(errors_array):.2f} meters\")\n",
        "print(f\"Standard Deviation: {np.std(errors_array):.2f} meters\")\n",
        "print(f\"Minimum Error: {np.min(errors_array):.2f} meters\")\n",
        "print(f\"Maximum Error: {np.max(errors_array):.2f} meters\")\n",
        "\n",
        "# Plot histogram of errors\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(errors_array, bins=30, color='blue', alpha=0.7, edgecolor='black')\n",
        "plt.title(\"Distribution of Errors\")\n",
        "plt.xlabel(\"Error (meters)\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Plot boxplot of errors\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.boxplot(errors_array, vert=False, patch_artist=True, boxprops=dict(facecolor='orange'))\n",
        "plt.title(\"Boxplot of Errors\")\n",
        "plt.xlabel(\"Error (meters)\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Cumulative distribution function (CDF) plot\n",
        "sorted_errors = np.sort(errors_array)\n",
        "cdf = np.arange(len(sorted_errors)) / float(len(sorted_errors))\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(sorted_errors, cdf, marker='.', linestyle='none', color='green')\n",
        "plt.title(\"Cumulative Distribution of Errors\")\n",
        "plt.xlabel(\"Error (meters)\")\n",
        "plt.ylabel(\"CDF\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv (3.13.0)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
